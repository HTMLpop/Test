name: Weekly Human Link Check (FAST)

on:
  schedule:
    - cron: "0 9 * * 1"   # Mondays 09:00 UTC
  workflow_dispatch:

jobs:
  check:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        shard: [0, 1, 2, 3, 4, 5, 6, 7]   # 8 parallel shards
    env:
      INPUT_FILE: Test/combined_master_with_urls.xlsx
      SHEET: Sheet1
      SHARDS: 8
      REPO_NAME: Test
      URL_COLS: "Masking Forms_URL,Fraud/Alerts_URLs,Public Records Request Form_URLs"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Resolve input path and convert Excel->CSV if needed
        run: |
          python - <<'PY'
          import os, pandas as pd, pathlib, shutil
          repo_name = os.environ.get("REPO_NAME","")
          raw = os.environ["INPUT_FILE"]

          candidates = [raw]
          parts = pathlib.PurePosixPath(raw).parts
          if parts and repo_name and parts[0].lower() == repo_name.lower():
              candidates.append("/".join(parts[1:]))
          candidates.append(os.path.basename(raw))

          chosen = None
          for c in candidates:
              if c and os.path.exists(c):
                  chosen = c
                  break

          if not chosen:
              raise FileNotFoundError(f"Could not find any of: {candidates}")

          print(f"Using input file: {chosen}")

          if chosen.lower().endswith((".xlsx",".xls")):
              sheet = os.environ.get("SHEET") or None
              df = pd.read_excel(chosen, sheet_name=sheet)
              df.to_csv("urls.csv", index=False)
              print("Converted Excel to urls.csv")
          else:
              shutil.copyfile(chosen, "urls.csv")
              print("Copied CSV to urls.csv")
          PY

       - name: Inspect input headers (debug)
         run: |
            python - <<'PY'
            import pandas as pd, re
            df = pd.read_csv('urls.csv')
            def norm(s): return re.sub(r'[^a-z0-9]+',' ', str(s).lower()).strip()
            print('Columns:', list(df.columns))
            print('Normalized keys:', [norm(c) for c in df.columns])
            print('Rows:', len(df))
            print(df.head(3).to_string())
            PY


      - name: Split into shards
        run: |
          python - <<'PY'
          import os, pandas as pd
          shards = int(os.environ['SHARDS'])
          shard  = int(os.environ['MatrixShard'])
          df = pd.read_csv('urls.csv').reset_index(drop=True)
          sub = df[df.index % shards == shard]
          out = f'urls_shard_{shard}.csv'
          sub.to_csv(out, index=False)
          print(f'Shard {shard}: {len(sub)} rows -> {out}')
          PY
        env:
          MatrixShard: ${{ matrix.shard }}

      - name: Run checker (shard; fast-only)
        run: |
          python human_link_check.py \
            --input urls_shard_${{ matrix.shard }}.csv \
            --concurrency 24 \
            --timeout-ms 10000 \
            --output-csv shard_${{ matrix.shard }}_results.csv \
            --output-json shard_${{ matrix.shard }}_results.json \
            ${URL_COLS:+--url-cols "$URL_COLS"}

      - name: Upload artifacts (per shard)
        uses: actions/upload-artifact@v4
        with:
          name: human-link-check-${{ matrix.shard }}
          path: |
            shard_${{ matrix.shard }}_results.csv
            shard_${{ matrix.shard }}_results.json
          retention-days: 7

  aggregate:
    runs-on: ubuntu-latest
    needs: check
    steps:
      - name: Download all shard artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: human-link-check-*
          merge-multiple: true

      - name: Merge shard results (no pandas)
        run: |
          python - <<'PY'
          import csv, glob, json

          csvs = sorted(glob.glob('shard_*_results.csv'))
          if csvs:
            headers = None
            rows = []
            for c in csvs:
              with open(c, newline='', encoding='utf-8') as f:
                r = csv.DictReader(f)
                if headers is None:
                  headers = r.fieldnames
                for row in r:
                  rows.append(row)
            with open('all_results.csv', 'w', newline='', encoding='utf-8') as f:
              w = csv.DictWriter(f, fieldnames=headers)
              w.writeheader()
              w.writerows(rows)
            print(f"Merged {len(csvs)} CSVs -> all_results.csv with {len(rows)} rows")
          else:
            print("No shard CSVs found")

          jsons = sorted(glob.glob('shard_*_results.json'))
          big = []
          for j in jsons:
            with open(j, 'r', encoding='utf-8') as f:
              big += json.load(f)
          if big:
            with open('all_results.json', 'w', encoding='utf-8') as f:
              json.dump(big, f, ensure_ascii=False, indent=2)
            print(f"Merged {len(jsons)} JSONs -> all_results.json with {len(big)} entries")
          else:
            print("No shard JSONs found")
          PY

      - name: Upload combined results
        uses: actions/upload-artifact@v4
        with:
          name: human-link-check-all
          path: |
            all_results.csv
            all_results.json
